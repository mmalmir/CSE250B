\listfiles
\documentclass[twoside,12pt]{article}
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage[top=2in, bottom=1.5in, left=0.85in, right=0.5in]{geometry}
\usepackage[hyphenbreaks]{breakurl}
%\usepackage[pdfstartview=FitH,pdfstartpage=13,pdfpagemode=UseNone]{hyperref}
\usepackage{amsfonts}
\usepackage{graphicx} 
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{float}
\usepackage{amssymb,amsmath}
\usepackage{mdwlist }
\usepackage{color}
\usepackage{multirow}
\usepackage{listings}
\usepackage{float}

\definecolor{darkblue}{rgb}{0.0,0.0,0.5}
\newtheorem{Dfn}{Definition}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
\newcommand{\sign}{\text{sign}}
\newcommand{\argmin}{\arg\!\max}
\begin{document}

\title{ Conditional Random Fields for Punctuation Prediction\\  Learning Algorithms, Project 2}
\author{Mohsen Malmir, Erfan Sayyari}
\maketitle
\section{Abstract}

\section{Introduction}
In this paper, we provide the results and details of implementation of a Conditional Random Field (CRF) model for predicting English language text punctuations. CRFs are a variant of undirected graphical models that are well suited for predicting structured labels. We train our model by maximizing log conditional likelihood (LCL) of the training data. During our experiments, we found that the model could predict individual tags by more than 94\% accuracy. However, as we performed more and more experiments, we discovered the over fitting to some prevalent tags in the training set. Using several experiments and methods that will be described later in this paper, we could overcome the over-fitting problem to some degrees.\\
We choose two techniques to train the proposed model: Collin's Perceptron and Contrastive Divergence. Both of these methods are approximations to the general gradient following method. We choose these two methods because of their two characteristics: simplicity and elegance. Both Collin's Perceptron and Contrastive Divergence provides simplified updates rule for the parameters, which are less expensive compared to the general gradient following for maximizing LCL. Despite simplicity, both methods have very clear and intuitive explanation behind them: they maximize the LCL of the data by throwing spurious samples at the model. However, as we explain our experiments in more details, we do not stick to the basic algorithms and tweak them in several ways to improve their performance.\\
One of the most important parts of the CRF model for predicting text punctuation is the choice of feature functions as it effects both the performance and accuracy of the model. The feature functions we develop here use Part-of-Speech (POS) tags of the input sentence. The main characteristic for the proposed feature functions is their efficiency in computation time: training of our method only takes a few minutes on a single core machine. We show that using these feature functions, the model can predict the punctuation tags with high accuracy.\\ 
For the given training and test data, danger of over-fitting is high because of the imbalanced distribution of punctuation tags. We deal with this problem with different techniques. We show how bounding the weight parameters and early stopping can improve the accuracy for the Collin's Perceptron. By introducing random sampling and guided sampling, we achieve similar improvements in Contrastive Divergence. Then we introduce the Turn-taking train procedure: for each tag we train a different predictor and then combine them using .... We show this last technique is much more effective and achieves the highest accuracy for punctuation prediction for the given data set.


\section{Design and Analysis of Algorithm}
\subsection{The general log-linear model}
Generally log-linear model is an extension of logistic regression. Conditional random Fields (CRFs) are a special case of log-linear models as well. Consider $x$ as an example that could be drawn from set $X$, and if we consider $y$ as a label which could be chosen from set of labels $Y$. In log-linear model we write the conditional probability $p(y|x;w)$ as:
\begin{equation}
p(y|x;w)=\frac{exp\sum_{j=1}^Jw_jF_j(x,y)}{Z(x,w)}.
\end{equation}

where $Z(x,y)$ is a normalization factor that is equal to:
\begin{equation}
Z(x,w)=\sum_{y' \in Y}exp\sum{j=1}{J}w_jF_j(x,y').
\end{equation}
In above equations $F_j(x,y)$ is called a feature function. If $w_j>0$ then observing a positive value for a feature function makes label y more probable to be label of x (other things fixed). Conversely, if $w_j<0$, observing a positive value for a feature function makes label $y$ less probable to be the label of x.

If we have weights $w_j$ and feature functions $F_j(x,y)$s, in order to assign a label to a test example x, we have to solve an argmax problem. Mathematically we could write it as:
\begin{equation}
\hat y = arg\max_y p(y|x;w)=arg\max_y\sum_{j=1}^Jw_jF_j(x,y).
\end{equation}
Above formula is called softmax function which is  a differentiable and convex function.

\subsection{Feature functions}
In general, a feature function can be a mapping from data space $X$ and label space Y to real-valued numbers $\mathbb{R}$, $F_j: X\times Y \rightarrow \mathbb{R}$. In our project, $F_j$ is a mapping from data space and label space to Boolean space, $F_j: X\times Y \rightarrow \{0,1\}$.

Usually, we define classes of feature functions using a template. We define $F_j(x,y)$ as the sum of some local feature functions, $f_j$. We could write:
\begin{equation}
F_j(x,y)=\sum_{i=1}^{n+1}f_j(y_{i-1}, y_i, x_{i-1}, x_i).
\end{equation} 
where in our project $x_i$ and $x_{i-1}$ are words $i$ and $i-1$ of the sentence and $f_j$ is an indication function. Instead of using each word in above equation we use Part-of-Speech (POS) tags of each word, so we could rewrite above equation as:
\begin{equation}
F_j(x,y)=\sum_{i=1}^{n+1}f_j(y_{i-1}, y_i, POS(x_{i-1}),POS( x_i)).
\end{equation} 
POS tags are corresponding part of speech of each word in a sentence, for example adjective, noun, verb and etc. In the next part we investigate them more. 

Above model has some problem, for example in practice many effective features depends on different positions of the sentence not consecutive ones. This model just considers POS tags of just two words that are consecutive. In addition, in above equation we do not consider different length of different sentence. We design feature functions in this way to avoid higher complexities that makes our problem unsolvable. 

\subsubsection{Part of Speech tagging}
In linguistics, part-of-speech tagging is the process of assigning a part of speech to a word. There are many packages in python which do POS tagging. In this project we use \emph{topia.termextract 1.1.0} ~\ref{cite:1} to do this preprocessing task. This package will assign different tags to different words of the sentence. The list of tags and their definition are represented in Table ~\ref{table:1}. Beside tags that are represented in Table ~\ref{table:1}, there are some other tags that if \emph{topia} does not know what it is consider it as a separate POS tag. In our project there are some of these unknown tags, for example \$, " ", etc. 

In order to extract POS tags, first we have to tokenize the sentence. Due to unknown tags and words in the dataset, we tokenize the sentence based on space first. Then we use \emph{topia.termextract 1.1.0} to assign tags to words. It does not take much time, and this package is fairly fast. 

The number of tags in Table ~\ref{table:1}, are large, which makes our algorithm so slow. In order to reduce them we use closed class words. We have to merge some of them into each other without any significant drawback on our problem. The set of simplified POS tags are represented in Table ~\ref{table:2}. We merge different types of nouns, adjectives, words start with wh (other than adverbs form because they are important for question mark), verbs, and pronouns into one group. Furthermore, we merge other unknown labels as Noise as well. In addition we merge determiner and predeterminer into adjectives too. (they related to nouns so we merge them into adjectives)
\begin{table}
\vspace{-2cm}
\center
\begin{tabular}{|c|c|c|}
\hline
 & Word tag & Meaning \\ \hline
1 & CC & Coordinating conjunction\\ \hline
2 & CD & Cardinal number \\ \hline
3 & DT & Determiner\\ \hline
4 & EX & Existential there\\ \hline
5 & FW & Foreign word \\ \hline
6 & IN & Preposition or subordinating conjunction \\ \hline
7 & JJ & Adjective \\ \hline
8 & JJR & Adjective, comparative \\ \hline
9 & JJS & Adjective, superlative \\ \hline
10 & LS & List item marker \\ \hline
11 & MD & Modal \\ \hline
12 & NN & Noun, singular or mass \\ \hline
13 & NNS & Noun, plural \\ \hline
14 & NNP & Proper noun, singular \\ \hline
15 & NNPS & Proper noun, plural \\ \hline
16 & PDT & Predeterminer \\ \hline
17 & POS & Possessive ending \\ \hline
18 & PRP & Personal pronoun\\ \hline
19 & PRP\$ & Possessive pronoun \\ \hline
20 & RB & Adverb \\ \hline
21 & RBR & Adverb, comparative \\ \hline
22 & RBS & Adverb, superlative \\ \hline
23 & RP & Particle \\ \hline
24 & SYM & Symbol \\ \hline
25 & TO & to \\ \hline
26 & UH & Interjection \\ \hline
27 & VB & Verb, base form \\ \hline
28 & VBD & Verb, past tense \\ \hline
30 & VBG & Verb, gerund or present participle \\ \hline
31 & VBN & Verb, past participle \\ \hline
32 & VBP & Verb, non-3rd person singular present \\ \hline
33 & VBZ & Verb, 3rd person singular present \\ \hline
34 & WDT & Wh-determiner \\ \hline
35 & WP & Wh-pronoun \\ \hline
36 & WP\$ & Possessive wh-pronoun \\ \hline
37 & WRB & Wh-adverb \\ \hline
\end{tabular}
\caption{topia.termextract 1.1.0 POS tags and their meanings}
\label{table:1}
\end{table}

\begin{table}
\vspace{-2cm}
\center
\begin{tabular}{|c|c|c|c|}
\hline
 & Simplified Word tag & Members & Meaning  \\ \hline
1 & ADJ & JJ, JJS, JJR, PDT, DT& Adjective\\ \hline
2 & PRP & PRP, PRP\$ & Pronoun  \\ \hline
3 & NN & NN, NNS, NNP, NNPS, FW & Nouns\\ \hline
4 & VB & VBD, VBG, VBP, VBN, VBZ, VB & Verbs\\ \hline
5 & WH & WP, WP\$, WDT & Wh-words \\ \hline
6 & RB & RB, RBR, RBS & Adverb \\ \hline
7 & Noise & \#, ' ', \$, (, ) &Unknown \\ \hline
8 & POS & POS& Possessive ending \\ \hline
9 & WRB & WRB& Wh-adverb \\ \hline
10 & CC & CC& Coordinating conjunction \\ \hline
11 & CD & CD&Cardinal number \\ \hline
12 & IN & IN&Preposition or subordinating conjunction\\ \hline
13 & EX & EX&Existential there \\ \hline
14 & MD & MD&Modal \\ \hline
15 & SYM & SYM&Symbol  \\ \hline
16 & UH & UH&Interjection \\ \hline
\end{tabular}
\caption{Closed class used as POS tags instead of original POS tags}
\label{table:2}
\end{table}
\subsection{Conditional random fields}
A linear conditional random Þeld is a way to apply a log-linear model to the task where we have different sequence of words with different length. The standard log-linear model is:

\begin{equation}
\hat y = arg\max_y p(y|x;w)
\end{equation}
for each training example x. Since the number of label tag sequences are exponential this task is very complex. Restricting the problem to just two adjacent POS tags and two consecutive labels makes  the problem much less easier to solve. Moreover, in this task we maximize log conditional likelihood (LCL) with regularization instead of original problem. 
\subsection{ Inference algorithms for linear-chain CRFs}
In order to solve the argmax problem, we can ignore the regularization factor since it is constant. Mathematically, we could write the problem as:

\begin{equation}
\hat y = arg\max_{\bar{y}} p(\bar{y}|\bar{x};w)=arg\max_{\bar{y}}\sum_{j=1}^{J}w_jF_j(\bar{x},\bar{y}).
\end{equation}
Using $F_j=\sum_{i=1}^{n+1}f_j(y_i,y_{i-1},POS(x_i),POS(x_{i-1})$ we could rewrite the objective function as:
\begin{align}
\hat y = arg\max_{\bar{y}} \sum_{j=1}^{J}w_j\sum_{i=1}^{n+1}w_jf_j(y_i,y_{i-1},POS(x_i),POS(x_{i-1}) \\
= arg\max_{\bar{y}}\sum_{i=1}^{n+1}g_i(y_{i-1},y_i).
\end{align}
where we define:
\begin{equation}
g_i(y_{i-1},y_i)=\sum_{j=1}^{J}w_jf_j(y_{i-1},y_i).
\end{equation}
Supposing that we have $\bar{x}$, $w$, and $i$  computing $g_i$ needs $O(m^2J)$ time, if we assume that we have $m$ different labels including $STOP$ and $START$ which show start and stop of the sentence. $U(k,v)$ is defined as the maximum of sum over $g_i$s from $i=1$ to $k$ such that the label of $k^{th}$ tag is $v$. This is equal to:

\begin{equation}
U(k,v)=\max_{y_1,...y_{k-1}}\sum_{i=1}^{k-1}g_i(y_{i-1},y_i) + g_k(y_{k-1},v).
\end{equation} 
This equation could be rewrite as:
\begin{align}
U(k,v)=\max_{y_{k-1}}\max_{y_1,...y_{k-2}}\sum_{i=1}^{k-2}g_i(y_{i-1},y_i) + g_k(y_{k-2},y_{k-1})+g_k(y_{k-1},v)\\
=\max_u[U(k-1,u)+g_k(u,v)].
\end{align}
where $y_{k-1}$ is equal to $u$. We need to compute $\max U(k-1,u)$ for $m$ different u, so computing $U(k,v)$ requires $O(m)$ time, if $v$ is fixed. Therefore, we can compute $U(k,v)$ for different $v$s in $O(m^2)$ time. Lastly, we can find best label for the entire sentence by:
\begin{equation}
\hat y_n=arg\max_vU(n,v).
\end{equation}
 In the above analysis we assume that $g_i$s are known. Considering these $g_i$s for a sentence $\bar{x}$ of length $n$, we can compute optimal $\hat y$ in $O(m^2nJ+m^2n)$.
\subsection{Gradients for log-linear models}
In order to train a log-linear model we have to find $w_j$s that maximize the objective function which is the conditional probability of labels given training examples. To solve maximization problem we calculate derivative of objective function with respect to parameters and set them zero. Since our problem is convex, this gives us the best answer. It is useful to note that instead of solving the main problem we solve LCL. Consequently, we can write:
\begin{align}
\frac{\partial}{\partial w_j}logp(y|x;w)=F_j(x,y)-\frac{\partial}{\partial w_j}logZ(x,w) \\
= F_j(x,y)-\frac{\partial}{\partial w_j}Z(x,w).
\end{align}
where $y$ is the known true label of the training example $x$.
\begin{equation}
\frac{\partial}{\partial w_j}Z(x,w)=\frac{\partial}{\partial w_j}\sum_{y'}[exp\sum_{j'}w_{j'}F_{j'}(x,y')]
\end{equation}
where $y'$ is different candidate labels. So we have:
\begin{equation}
\frac{\partial}{\partial w_j}Z(x,w)=\sum_{y'}[exp\sum_{j'}w_{j'}F_{j'}(x,y')]F_j(x,y')
\end{equation}
And lastly we have:
\begin{align}
\frac{\partial}{\partial w_j}logp(y|x;w)=F_j(x,y)-\frac{1}{Z(x,w)}\sum_{y'}F_j(x,y')[exp\sum_{j'}w_{j'}F_{j'}(x,y')]\\
= F_j(x,y)-\sum_{y'}F_j(x,y')[\frac{exp\sum_{j'}w_{j'}F_{j'}(x,y')}{Z(x,w)}].
\end{align}
Considering that $[\frac{exp\sum_{j'}w_{j'}F_{j'}(x,y')}{Z(x,w)}] = p(y'|x;w)$ we can simplify above equation to:
\begin{align}
\frac{\partial}{\partial w_j}logp(y|x;w)=F_j(x,y)-\sum_{y'}F_j(x,y')p(y'|x;w)\\
= F_j(x,y)-E_{y'~p(y'|x;w)}[F_j(x,y')].
\end{align}
Above equation is just for an example of training set, and for the entire training set, $T$, we could rewrite it as:
\begin{equation}
\sum_{<x,y>\in T}F_j(x,y)=\sum_{<x,.>\in T}E_{y~p(y|x;w)}[F_j(x,y)].
\end{equation}
\subsection{Stochastic gradient ascent}
Usually, we calculate the best $w_j$ by gradient ascent method, which is equal to:
\begin{equation}
w_j \leftarrow w_j+\sum_{<x,y>\in T}F_j(x,y)=\sum_{<x,.>\in T}E_{y~p(y|x;w)}[F_j(x,y)]
\end{equation} 
where $\lambda$ is learning factor. Updating $w_j$ using above equation is very time consuming so in practice we use stochastic gradient methods considering just one random sample at each time as follows:
\begin{equation}
w_j \leftarrow w_j+\lambda(F_j(x,y)-E_{y'~p(y'|x;w)}[F_j(x,y')])
\end{equation}
In this way, the total time complexity of the updates for all $j$, for a single training $x$ and its label $y$, is $O(Jm^2n)$.
\subsection{The Collins perceptron}
Suppose that $E_{y'~p(y'|x;w)}[F_j(x,y')]=F_j(x,\hat y)$ where $\hat y = arg\max_y p(y|x;w)$, so we can rewrite the stochastic gradient update rule as:
\begin{equation}
w_j\leftarrow w_j+\lambda F_j(x,y)-\lambda F_j(x,\hat y).
\end{equation}
This method is called Collins perceptron. It is useful to mention that multiplying all $w_j$s by the same factor does not affect on finding label $\hat y$ which has highest probability. So we could simply set $\lambda$ to be equal to 1.
\subsubsection{Gibbs sampling}
Instead of assigning $E_{y'~p(y'|x;w)}[F_j(x,y')]=F_j(x,\hat y)$ we can compute $E_{y'~p(y'|x;w)}[F_j(x,y')]$ by sampling $y$ values from distribution $p(y|x;w)$. To do this we can use Gibbs sampling. if we write $y$ as a set with its sub-tags as $y=\{y_1,....y_n\}$, and if we suppose that we have all the conditional distributions:
\begin{equation}
p(Y_i=v|x,y_1,...,y_{i-1},y_{i+1},....y_n;w)
\end{equation}
then we can draw a stream of samples as: \\
1) select an arbitrary initial guess $\{y1,.....,y_n\}$ \\
2) Draw a new value for $y_1$ from distribution $p(Y_1|x,y_2,...,y_n;w);$ \\
Draw a new value for $y_1$ from distribution $p(Y_1|x,y_2,...,y_n;w);$ \\
Draw a new value for $y_2$ from distribution $p(Y_2|x,y_1,y_3,...,y_n;w);$ \\
continue above procedure till finding $y_n$ from distribution $p(Y_n|x,y_1,y_2,...,y_{n-1};w);$
3)Repeat 2. It could be proved that repeating step 2 infinitely, we converge to the distribution p(y|x;w).

We can calculate distribution $p(v|x,y_{-i};w)$ by:
\begin{equation}
p(v|x,y_{-i};w)=\frac{[exp g_i(y_{i-1},v)][exp g_{i+1}(v,y_{i+1})]}{\sum_{v'}[exp g_i(y_{i-1},v')][exp g_{i+1}(v',y_{i+1})]}
\end{equation}
Consider doing one round of Gibbs sampling, requires $O(mn)$ time.
\subsection{Contrastive divergence}
Contrastive divergence method tries to find a single $y^*$ similar to true label y, with higher probability. We implement contrastive divergence by Gibbs sampling which is equal to finding a $y^*=<y_1^*,y_2^*,...,y_n^*>$ by usually one round of Gibbs sampling, starting from true label as step one.
\section{Design of Experiments}

To test the proposed model, we perform several experiments on the given dataset, which is an English language punctuation dataset that consists of 70115 training and 28027 test sentences. We use only training data to fit the model,  one third for validation and two thirds for training.	 After training is complete, the test data is used to assess prediction and generalization of the model. Each sentence in the training data consists of a set of words without any punctuation symbols. For multiple sentences, there was a few form of syntax complications such as use of '\$' in place of the word 'money', or words that were split to two parts by space *****. We ignored all such cases, as they were few sentences and finding a complete list of them required going through the entire dataset which was very time consuming.\\

\subsection{Feature extraction}
Our features are of the form $F_j(x,y)=\sum_{i=1}^{n+1}f_j(y_{i-1}, y_i, POS(x_{i-1}),POS( x_i))$, where $y_i$ and $y_{i-1}$ denote labels in positions $i$ and $i-1$ of the sentence and $POS(x_i)$ and $POS(x_{i-1})$ denotes POS tag of sentence in positions $i$ and $i-1$. So each small feature function $f_j$ depends on two different POS tag and two different label. In other words we have J different feature functions that $J=m^2k^2$ where $m$ is equal to number of labels and $k$ is equal to number of POS tags. Since our feature functions are indication functions, they are just non-zero for especial POS tags. It is useful to mention that actual number of feature functions are far smaller than $J$ since many of combinations of POS tags do not happen in our training set. In order to implement it more efficiently, we assign each combination of POS tags an index, and we assign each label an index too. In overall we assign each combination of $\{y_{i-1}, y_i, POS(x_{i-1}),POS( x_i)\}$ an index which is equal to the index for the weight $w_j$ associated with them too. Consequently, if we want to compute $F_j(x,y)$ we count number of observations of the combination $\{y_1, y_2, POS(x_{1}),POS( x_2)\}$ along the sentence and put it inside the memory associated with their index. To update $w_j$ we use this $F_j$ and update it. 

In order to calculate a unique index for each combination we use the following formula:
\begin{align}
idx = idx_{POS_2}*(number of labels)^2(number of POS tags)\\
+idx_{POS_1}*(number of labels)^2+idx_{label_2}*(number of labels)+idx_{label_1}.
\end{align}

The other important thing about our feature functions is that, we increase the length of sentences to become the same, equal to $d$. In shorter sentence after the $STOP$ we have -1, for $STOP$ we assign zero and to $START$ we assign $d+1$ ($d$ is the maximum length of sentence in training data).

\subsection{Initialization}
For Collin's Perceptron, as pointed out in the lecture, the learning rate can be fixed to $\lambda=1$. This is because scaling scaling the weight vector $w$ does not affect the predicted label by the model. Using different learning rates leads to different weight vectors that are scaled versions of each other. However, one should note that this is correct only if the model converges to the global optimum ******. For both experiments on Collin's Perceptron and Contrastive Divergence, we fix $\lambda \leftarrow 1$. \\
For initializing the weight vector $w$, we use small random Normal numbers from $\mathcal N(0.,1e-5)$. This is because as we describe later, limiting the weight vector entries $w_j$ to be in a small range $[-a,a]$ helps to prevent over-fitting. Because both $a$ and $-a$ represents the extreme learned situation, a good option for initial weights is a random vector that is not biased for specific feature functions, that is $w_j$ better be close to 0 for all $j$.\\
\subsection{Preprocessing}
\subsection{Performance Measure}
We report the performance of the models in two different ways. First, we report the prediction of individual target tags, That is, how accurate the model can predict the individual punctuations tags in corpus sentences. We report the accuracy of prediction for the entire tags as wells as individual target tags. The benefit of looking into individual punctuation tag prediction is that it reveals if over-fitting has happened due to imbalanced distribution of tags in the training data. Next, we report the accuracy of model for predicting sentence punctuation. The accuracy for this case should be lower because each sentence punctuation is composed of multiple tags. 
\section{Results of Experiments}
\subsection{Collins perceptron}
For training Collin's Perceptron, we divide the training data into two thirds for training and one third for validation. The initial value for the weight vector entries $w_j$ is chosen to be small random numbers from $\mathcal{N}(0,1e-5)$. We use early stopping as a criteria for terminating the training procedure. Because Collin's Perceptron has no hyper parameters, there is a chance of over fitting to the training data. In the train procedure, we divide the data into train and validation sets and start training only with train data. After each epoch, we measure the accuracy for predicting individual punctuation tags on the validation data. If the accuracy has decreased, we stop the training and roll back the weights to previous values.\\
For the first experiment, we train Collin's perceptron with early stopping. Figure \ref{figEarlyStoppingPercGibbs} shows that the training stops after 4 epochs. However, after the first epoch, the model is almost converged and the accuracy changes a small amount after that. The accuracy for predicting individual tags and sentences for validation and test sets are shown in Table \ref{tablePredPerceptron}.\\

\begin{figure}[H]
\centering
\includegraphics[width=.7\textwidth]{./figs/Perceptron/Perceptron.png}
\caption{Early stopping for Collin's Perceptron. }
\label{figEarlyStoppingPercGibbs}
\end{figure}

\begin{table}[H]\footnotesize
  \caption{Collin's Perceptron performance on the given data set}
\begin{center}
    \begin{tabular}{| c | c | c | }
    \hline
    \textbf{ Measure} & \textbf{Validation set accuracy} & \textbf{Test set accuracy} \\ \hline
    Individual tag prediction & 0.911 & 0.901  \\ \hline
    Sentence level prediction & 0.389 & 0.371  \\ \hline
    \end{tabular}
    \label{tablePredPerceptron}
\end{center}
\end{table}


In order to further assess the correctness of the model, we look into the accuracy for predicting individual tags, that is, how much the model is accurate in predicting different punctuation tags. Table \ref{tableIndividualTagsPerceptron} shows the accuracy for predicting individual tags. Figure \ref{figPerceptronConfMat} shows the confusion matrix for the same prediction problem. A quick look at table \ref{tableIndividualTagsPerceptron} and figure \ref{figPerceptronConfMat} reveals some properties of the Collin's Perceptron model. Here, the model has learned only the most frequent punctuation tags in sentences, that is SPACE and PERIOD. For each test sentence the model predicts a simple punctuation tag sequence which has SPACE or PERIOD in most positions (look at SPACE and PERIOD columns in the test confusion matrix in figure \ref{figPerceptronConfMat}). This behavior is somehow natural, because in training phase, the model sees sequences of tags with SPACE or PERIOD more often, and thus the corresponding weights to these weights are more frequently updated. Table \ref{tablePerceptronWeights} shows 5 largest entries in the weight vector and their corresponding feature function (******* DESC FOR FFs).

\begin{table}[H]\footnotesize
  \caption{Accuracy of predicting different tags by Collin's Perceptron}
\begin{center}
    \begin{tabular}{| c | c | c | c | c |}
    \hline
    \textbf{ Method} & \multicolumn{2}{ c| }{\textbf{Validation set}}  & \multicolumn{2}{ c| }{\textbf{Test set}} \\ \hline
     & Accuracy & Frequency & Accuracy & Frequency \\ \hline
    EXCLAMATION\_POINT & 0.0 & 2567 & 0.0 & 1066  \\ \hline
    SPACE & 0.947 & 580077  & 0.947 & 235536\\ \hline
    QUESTION\_MARK & 0.145 & 10904 & 0.1444 & 4792 \\ \hline
    PERIOD & 0.849 & 57259  & 0.847 & 22492\\ \hline
    COLON & 0.009 & 1009  & 0.003 & 1174\\ \hline
    COMMA & 0.248 & 28555  & 0.262 & 10665\\ \hline
    \end{tabular}
    \label{tableIndividualTagsPerceptron}
\end{center}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=1.\textwidth]{./figs/figPerceptronConfMat.png}
\caption{Confusion matrix for predicting individual tags using Collin's Perceptron on Left: validation, and Right: test sets.}
\label{figPerceptronConfMat}
\end{figure}


\begin{table}[H]\footnotesize
  \caption{20 Largest entries of the weight vector  for Collin's Perceptron after learning. For each weight entry, its magnitude and corresponding feature function is listed.}
\begin{center}
    \begin{tabular}{| c | c | c | c | c |}
    \hline
    \textbf{ Magnitue} & $X_{i-1}$ & $X_i$ & $Y_{i-1}$& $Y_i$ \\ \hline
3964.99 & NN & VB & COMMA & SPACE  \\ \hline
3927.99 & NN & VB & SPACE & SPACE  \\ \hline
3308.99 & ADJ & NN & SPACE & COMMA  \\ \hline
2875.99 & IN & NN & SPACE & COMMA  \\ \hline
2460.99 & PRP & NN & SPACE & COMMA  \\ \hline
2099.00 & NN & IN  & COMMA & SPACE  \\ \hline
2067.00 & NN & IN & SPACE & SPACE  \\ \hline
1800.99 & NN & NN & COMMA & SPACE  \\ \hline
1771.99 & PRP & VB & SPACE & COMMA  \\ \hline
1759.00 & NN & NN & SPACE & SPACE  \\ \hline
1508.99 & NN & WRB & COMMA & SPACE  \\ \hline
1463.00 & NN & WRB & SPACE & SPACE  \\ \hline
1451.00 & NN & PRP & COMMA & SPACE  \\ \hline
1409.00 & NN & PRP & SPACE & SPACE  \\ \hline
1209.99 & ADJ & PRP & COMMA & SPACE  \\ \hline
1198.99 & VB & ADJ & SPACE & COMMA  \\ \hline
1077.00 & ADJ & PRP & SPACE & SPACE  \\ \hline
1056.00 & NN & ADJ & COMMA & SPACE  \\ \hline
1018.99 & NN & ADJ & SPACE & SPACE  \\ \hline
997.000 & IN & PRP & SPACE & COMMA  \\ \hline
    \end{tabular}
    \label{tablePerceptronWeights}
\end{center}
\end{table}

\subsection{Contrastive Divergence}
For Contrastive Divergence, the training setup is the same as mentioned in previous section. The training data is divided into training and validation sets, and early stopping is applied to terminate the training. The initial value for the weight vector entries are chosen randomly from $\mathcal{N}(0,1e-5)$. 
\subsubsection{Training with Bounded Weights}
As we saw in Collin's Perceptron, the weight vector entries grow unbounded for tags that are more frequent in the training data. This unbounded growth can cause problems during prediction, because the large weights tend to bias the decision in favor of more frequent tags (e.g. SPACE or PERIOD). One way to deal with this situation is to use a separate learning rate for each entry in the weight vector. Then if a $w_j$ has been updated frequently, we can decrease its learning rate. A  similar approach is to force the $w_j$s to be in a specified range $[-a,a]$. For each update, one then should check that $w_j$ doesn't exceed this bound. We adapt this technique in training of Contrastive Divergence model, with hope that this reduces over-fitting to specific punctuation tags.\\
Figure \ref{figCDEarlyStopping} shows that early stopping terminates the training for Contrastive Divergence after two epochs. Tables \ref{tablePredGibbs} and \ref{tableIndividualTagsGibbs} show the performance for this model. These results are very interesting as we compare it row by row to table \ref{tableIndividualTagsPerceptron}. We see that the tag EXCLAMATION\_POINT, QUESTION\_MARK and COLON are learned more and accuracy of their prediction has  increased in Contrastive Divergence compared to Collin's Perceptron. Specifically this is interesting because these are the lowest frequency symbols in the training dataset. However, this learning occurred at the expense of \emph{unlearning} COMMA and PERIOD, which lead to decreased overall prediction accuracy both on sentence level and individual tag level prediction.  Table \ref{tableGibbsWeights} displays some of the weights learned by Contrastive Divergence. (*****DESC FOR FFs)

\begin{figure}[H]
\centering
\includegraphics[width=1.\textwidth]{./figs/Gibbs/CD.png}
\caption{Early stopping for Contrastive Divergence.}
\label{figCDEarlyStopping}
\end{figure}


\begin{table}[H]\footnotesize
  \caption{Contrastive Divergence performance on individual tag and sentence punctuation prediction.}
\begin{center}
    \begin{tabular}{| c | c | c | }
    \hline
    \textbf{ Measure} & \textbf{Validation set accuracy} & \textbf{Test set accuracy} \\ \hline
    Individual tag prediction & 0.866 & 0.852  \\ \hline
    Sentence level prediction & 0.113 & 0.115  \\ \hline
    \end{tabular}
    \label{tablePredGibbs}
\end{center}
\end{table}


\begin{table}[H]\footnotesize
  \caption{Accuracy of predicting different tags by Collin's Perceptron}
\begin{center}
    \begin{tabular}{| c | c | c | c | c |}
    \hline
    \textbf{ Method} & \multicolumn{2}{ c| }{\textbf{Validation set}}  & \multicolumn{2}{ c| }{\textbf{Test set}} \\ \hline
     & Accuracy & Frequency & Accuracy & Frequency \\ \hline
    EXCLAMATION\_POINT & 0.486 & 2567 & 0.518 & 1066  \\ \hline
    SPACE & 0.945 & 580077  & 0.947 & 235536\\ \hline
    QUESTION\_MARK & 0.249 & 10904 & 0.247 & 4792 \\ \hline
    PERIOD & 0.849 & 57259  & 0.847 & 22492\\ \hline
    COLON & 0.3 & 1009  & 0.5 & 1174\\ \hline
    COMMA & 0.086 & 28555  & 0.076 & 10665\\ \hline
    \end{tabular}
    \label{tableIndividualTagsGibbs}
\end{center}
\end{table}

\begin{table}[H]\footnotesize
  \caption{20 Largest entries of the weight vector  for Contrastive Divergence after learning.}
\begin{center}
    \begin{tabular}{| c | c | c | c | c |}
    \hline
    \textbf{ Magnitue} & $X_{i-1}$ & $X_i$ & $Y_{i-1}$& $Y_i$ \\ \hline
0.1& WRB& IN &SPACE& SPACE\\ \hline
0.1& NN &UH &SPACE &COLON\\ \hline
0.1& NN &UH &SPACE &PERIOD\\ \hline
0.1 &ADJ &WH &SPACE &SPACE\\ \hline
0.1 &PRP &ADJ &COMMA& SPACE\\ \hline
0.1 &PRP &ADJ &SPACE &QUESTION\_MARK\\ \hline
0.1 &PRP& ADJ &SPACE &SPACE\\ \hline
0.1 &IN &VB &SPACE &SPACE\\ \hline
0.1 &VB &IN &SPACE &SPACE\\ \hline
0.1 &VB &IN &SPACE &PERIOD\\ \hline
0.1 &VB &IN &SPACE &COMMA\\ \hline
0.1 &IN &VB &SPACE &QUESTION\_MARK\\ \hline
0.1 &IN &VB &SPACE &PERIOD\\ \hline
0.1 &NN &SYM &COMMA& SPACE\\ \hline
0.1 &CC &CD &SPACE &PERIOD\\ \hline
0.1 &CC &CD &SPACE &SPACE\\ \hline
0.1 &PRP &CD& SPACE &PERIOD\\ \hline
0.1 &PRP &CD &SPACE &QUESTION\_MARK\\ \hline
0.1 &PRP &EX &SPACE &QUESTION\_MARK\\ \hline
0.1 &PRP& EX &SPACE &PERIOD\\ \hline
    \end{tabular}
    \label{tableGibbsWeights}
\end{center}
\end{table}



\section{Findings and Lessons Learned}
\subsection{Numerical Issues and Preprocessing}
\subsection{Overfitting}
\subsection{Model Selection}
\subsection{Future works}
\section{References}
url = "https://pypi.python.org/pypi/topia.termextract/"
\end{document}