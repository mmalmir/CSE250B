\listfiles
\documentclass[twoside,12pt]{article}
\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage[top=2in, bottom=1.5in, left=0.85in, right=0.5in]{geometry}
\usepackage[hyphenbreaks]{breakurl}
%\usepackage[pdfstartview=FitH,pdfstartpage=13,pdfpagemode=UseNone]{hyperref}
\usepackage{amsfonts}
\usepackage{graphicx} 
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{float}
\usepackage{amssymb,amsmath}
\usepackage{mdwlist }
\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.5}
\newtheorem{Dfn}{Definition}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
\newcommand{\sign}{\text{sign}}
\newcommand{\argmin}{\arg\!\max}
\begin{document}

\title{Learning Algorithms, Project 2}
\author{Mohsen Malmir, Erfan Sayyari}
\maketitle
\section{Abstract}

\section{Introduction}
In this paper, we provide the results and details of implementation of a Conditional Random Field (CRF) model for predicting English language text punctuations. CRFs are a variant of undirected graphical models that are well suited for predicting structured labels. We train our model by maximizing the log conditional likelihood of the training data. During our experiments, we found that the model could predict individual tags by more than 94\% accuracy. However, as we performed more and more experiments, we discovered the over fitting to some prevalent tags in the training set. Using several experiments and methods that will be described later in this paper, we could overcome the over-fitting problem to some degrees.

 
\section{Design and Analysis of Algorithm}
\subsection{The general log-linear model}
\subsection{Feature functions}
\subsubsection{Part of Speech tagging}
\subsection{Conditional random fields}
\subsection{The Collins perceptron}
\subsection{Contrastive divergence}
\subsubsection{Gibbs sampling}
\section{Design of Experiments}
\subsection{Initialization}
\subsection{Preprocessing}
\subsection{Performance Measure}
\section{Results of Experiments}
\subsection{Collins perceptron}
\subsection{Gibbs sampling}

\section{Findings and Lessons Learned}
\subsection{Numerical Issues and Preprocessing}
\subsection{Overfitting}
\subsection{Model Selection}
\subsection{Future works}
\end{document}